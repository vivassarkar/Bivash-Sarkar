# GPT-2 model
We create a full implementation of GPT-2 from scratch, including all the details like multi-head self-attention, feed-forward networks, and positional encoding. Here we provide  with a simplified version of a GPT-2-like model using PyTorch. This implementation will not be as optimized or feature-complete as the original, but it should give you a basic understanding of the architecture.
GPT-2 models' robustness and worst case behaviors are not well-understood. As with any machine-learned model, carefully evaluate GPT-2 for our use case, especially if used without fine-tuning or in safety-critical applications where reliability is important.
I take helps from the official github GPT-2 of openAI.
